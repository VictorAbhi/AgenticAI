{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41db6f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adabh\\OneDrive\\Desktop\\AI Tasks\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# install necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0dc654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adabh\\AppData\\Local\\Temp\\ipykernel_21220\\2501627198.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# Load .env variables\n",
    "load_dotenv()\n",
    "\n",
    "# Access API key from environment\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Set up LLM and memory\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",temperature=0.7)\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c087d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentiment-aware prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are a friendly and empathetic chatbot. Adapt your tone based on the user's sentiment:\n",
    "- For positive sentiment, respond enthusiastically and warmly.\n",
    "- For negative sentiment, respond with empathy and offer support.\n",
    "- For neutral sentiment, respond informatively and politely.\n",
    "The user's current sentiment is {sentiment}. Use the conversation history to maintain context.\n",
    "\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee8ef03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis model\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "def get_sentiment(text):\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    label = result[\"label\"].lower()  # 'POSITIVE', 'NEGATIVE'\n",
    "    return label if label in [\"positive\", \"neutral\", \"negative\"] else \"neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6757004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment(\"I'm little bit sad today.\") # sentiment check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdacecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c70f0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_bot(user_input):\n",
    "    # Get sentiment of user input\n",
    "    sentiment = get_sentiment(user_input)\n",
    "    \n",
    "    # Load conversation history as a list of BaseMessage objects\n",
    "    history = memory.load_memory_variables({})[\"history\"]\n",
    "    \n",
    "    # Invoke the chain with sentiment and history\n",
    "    response = chain.invoke({\n",
    "        \"input\": user_input,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"history\": history  # Pass history as a list of BaseMessage objects\n",
    "    })\n",
    "    \n",
    "    # Explicitly save messages to memory\n",
    "    memory.chat_memory.add_message(HumanMessage(content=user_input))\n",
    "    memory.chat_memory.add_message(AIMessage(content=response.content))\n",
    "    \n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c971324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Oh, that's absolutely wonderful to hear! There's nothing quite like the energy and excitement of starting a new project. I'm so happy for you!\n",
      "\n",
      "I would love to hear all about it if you feel like sharing! What are you working on?\n"
     ]
    }
   ],
   "source": [
    "# Test the chatbot\n",
    "user_input = \"I'm so excited about my new project!\"\n",
    "response = chat_with_bot(user_input)\n",
    "print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5c72b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Oh no, I'm so sorry to hear you're feeling that way. It's completely okay to have down days, and I'm here for you.\n",
      "\n",
      "If you'd like to talk about what's on your mind, please know I'm here to listen without any judgment. There's no pressure at all, but sometimes just getting it out can help a little.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"I'm feeling a bit down today.\"\n",
    "response = chat_with_bot(user_input)\n",
    "print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4d274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
